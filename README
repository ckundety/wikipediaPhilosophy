
# get data
cd data
wget http://download.wikimedia.org/enwiki/20110722/enwiki-20110722-pages-articles.xml.bz2   # 7.1gb

# flatten to single line
bzcat enwiki-20110722-pages-articles.xml.bz2 | ~/flattenToOnePagePerLine.py > enwiki-20110722-pages-articles.pageperline.xml # 30gb

# split into redirects and articles
cat enwiki-20110722-pages-articles.pageperline.xml | grep \<redirect\ \/\> > enwiki-20110722-pages-redirects.xml &   
cat enwiki-20110722-pages-articles.pageperline.xml | grep -v \<redirect\ \/\> > enwiki-20110722-pages-articles.xml & 
wait

# move xml for articles and redirects into hdfs 
hadoop fs -mkdir /full/articles.xml
hadoop fs -copyFromLocal enwiki-20110722-pages-articles.xml /full/articles.xml &
hadoop fs -mkdir /full/redirects.xml
hadoop fs -copyFromLocal enwiki-20110722-pages-redirects.xml /full/redirects.xml &
wait

# parse redirects
cd
hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
 -input /full/redirects.xml -output /full/redirects \
 -mapper redirectParser.py -file redirectParser.py

# dereference all redirects to their final targets
pig -p INPUT=/full/redirects -p OUTPUT=/full/redirects.dereferenced1 -f dereference_redirects.pig
pig -p INPUT=/full/redirects.dereferenced1 -p OUTPUT=/full/redirects.dereferenced2 -f dereference_redirects.pig
pig -p INPUT=/full/redirects.dereferenced2 -p OUTPUT=/full/redirects.dereferenced3 -f dereference_redirects.pig
pig -p INPUT=/full/redirects.dereferenced3 -p OUTPUT=/full/redirects.dereferenced4 -f dereference_redirects.pig
hfs -mv /full/redirects /full/redirects.original
hfs -mv /full/redirects.dereferenced4 /full/redirects

# run extraction
hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
 -input /full/articles.xml -output /full/edges \
 -mapper articleParser.py -file articleParser.py

# run redirects against edges
pig -p INPUT=/full/edges -p OUTPUT=/full/edges.dereferenced -f dereference_redirects.pig

# as a sanity check, should be same
# pig -p INPUT=/full/edges.dereferenced -p OUTPUT=/full/edges.dereferenced2 -f dereference_redirects.pig 

# get to local filesystem
hadoop fs -cat /full/edges.dereferenced/* > data/edges

# inject special cases
cat special_edges_cases >> data/edges

# calculate distance from Philosophy
java -Xmx8g -cp . DistanceToPhilosophy Philosophy data/edges >DistanceToPhilosophy.stdout 2>DistanceToPhilosophy.stderr

# visited vs non visited
grep ^FINAL DistanceToPhilosophy.stdout | wc -l 
grep ^didnt\ visit DistanceToPhilosophy.stdout | wc -l

# work out which nodes we didn't visit
grep ^didnt DistanceToPhilosophy.stdout | sed -es/didnt\ visit\ // > didnt_visit
# summarise why we didn't visit them
./walk_till_end.py < didnt_visit > walk_till_end.stdout
grep end\ of\ line$ walk_till_end.stdout | cut -f2 | sort | uniq -c | sort -nr | head



San Jose InternationalSeattle-Tacoma International COMcast__11
tV_s6ALJ