distance to philosophy

http://xkcd.com/903/

not such a big deal

questions
1) do all wikipedia articles link to philosophy?
2) what distribution do the distances take?

method:
1) get wikipedia dump from volume
2) parse to make a graph; term -> term
3) connected components; is it one? if not which one is philosophy in?
4) histogram of distances

part 1) get wikipedia dump from volume

go to snapshot snap-1781757e Wikipedia Extraction-WEX (Linux)
from it make a volume vol-? (in, say,  us-east-1c)
make an instance; ebs backed
attach volume to instance  i-71941410 (also in us-east-1c)

device /dev/sdk

copy from ebs to hdfs

 mkdir wiki; 
 sudo mount /dev/xvdk wiki
 hadoop fs -mkdir /full/articles
 hadoop fs -copyFromLocal wiki/rawd/freebase-wex-2009-01-12-articles.tsv /full/articles_one_file # 7 min
 hadoop fs -mkdir /full/redirects
 hadoop fs -copyFromLocal wiki/rawd/freebase-wex-2009-01-12-redirects.tsv /full/redirects

and for testing...
 hadoop fs -mkdir /sample/articles
 head -n100 wiki/rawd/freebase-wex-2009-01-12-articles.tsv > sample
 hadoop fs -copyFromLocal sample /sample/articles/freebase-wex-2009-01-12-articles.tsv

the interesting file is wiki/rawd/freebase-wex-2009-01-12-articles.tsv
which is 31G; 4,183,153 articles

http://wiki.freebase.com/wiki/WEX/Documentation

it has 5 columns 
0 - id
1 - title
2 - date
3 - xml
4 - plain text

(maybe ignore this)
before going too far it'd be interesting to extract the actual list of titles..
 hadoop jar ~/contrib/streaming/hadoop-streaming.jar -input /sample/articles/ -output /sample/titles -mapper '/usr/bin/cut -f2' -numReduceTasks 0
b(maybe ignore this)

first need to split into chunks for in/out of s3 (with gzipping)

hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
 -D mapred.min.split.size=419430400 \
 -D mapred.output.compress=true \
 -D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec  \
 -input /full/articles_one_file/ -output /full/articles \
 -mapper /bin/cat -numReduceTasks 0 

reduces it to 79 files, 75mb each, 6gb total

after playing with the data and bit (and refining the algorithm) the basic heuristic is
 ignore until first <sentence>
 find first <target> that isn't article name (as often, the first one is)

of course it's not that simple, there are lots of extra special cases...

eg [File:BSicon ABZvlr.svg] [Category:AB Castell√≥n players] or [Template:Sharpness Branch Line]

what the distinct values of these?

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /sample/articles/ -output /sample/article_types \
  -mapper '/usr/bin/python metaArticleTypes.py' -file metaArticleTypes.py \
  -reducer aggregate

of course, nothing is clean :D

$ hfs -cat /full/article_types/*|sort -k2 -t'    ' -nr|head -n20
normal file		 2684331
File   862604
Category	434783
Template	164138
Portal		15543
Portal talk	1175
File talk	903
Category:Wikipedians by alma mater	739
Meanings of minor planet names		218
List of minor planets	 203
ISO 3166-2    198
List of United Kingdom locations	125
List of drugs  114
Star Wars      91
Star Trek      85
Template:Ph    83
Library of Congress Classification	83
Theme Time Radio Hour			81
Live Phish Downloads			74
Batman	   66

so maybe ignore; File, Category, Template, Portal, Portal talk, File talk
curious now, what do these represent?

$ hfs -cat /full/articles/part-00232.gz | gunzip | cut -f2 | grep ^File | shuf | head
File:TriGeo Logo.JPG
File:Tuckerdorothy.jpg
File:Hippoquarium.jpg
File:Gbridge-cap.jpg
File:Twoc.jpg
File:Eurovision 81.jpg
File:BMW 003 jet engine.JPG
File:Lochailort.jpg
File:New Zealand General Service Medal - Iraq.jpg
File:Pixiesheadon.jpg

$ hfs -cat /full/articles/part-00232.gz | gunzip | cut -f2 | grep ^Category | shuf | head
Category:User bcl-3
Category:Sport in Hamilton, New Zealand
Category:People murdered in Norfolk Island
Category:Top-importance Old-time Base Ball articles
Category:Calgary Mustangs players
Category:NA-Class Japanese baseball articles
Category:Deaths by firearm in Nebraska
Category:Irish folk-song collectors
Category:University of Maryland, Baltimore County faculty
Category:Human death in Nebraska

$ hfs -cat /full/articles/part-00232.gz | gunzip | cut -f2 | grep ^Portal | shuf | head
Portal:Furry/Did you know/2
Portal:Western Australia/Selected article/September 2008
Portal:Edgar Allan Poe/Selected picture/October
Portal:Tropical cyclones/Featured article/Monsoon trough
Portal:Spaceflight/On This Day/5 September
Portal:Philadelphia/Philadelphia news/September 2008
Portal:BBC/Selected article/2
Portal:Greater Manchester/Did you know/archive
Portal:Japan/Did you know/56
Portal talk:Trains/Anniversaries/August 30


 ignore until first <sentence>
 find first <target> that 
  isn't article name (as often, the first one is)
  doesn't start with File:, Category:, Template:, Portal:, Portal talk:, File talk:

sometimes we just need to ignore the "article" too.
i think when the 'plain text' (col[4]) is <100 characters it's probably a meta article too...

in fact this seems to be a more general case;

so... 
 ignore if plain_text < 100 chars
 ignore until first sentence
  find first <target> that isn't article name

there is also another interesting file is freebase-wex-2009-01-12-redirects.tsv
which i suspect will be required since sometimes the second target will require redirect dereference

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /sample/articles/ -output /sample/edges \
  -mapper '/usr/bin/python articleParser.py' -file articleParser.py
  -numReduceTasks 0


agassi should be 'List of ATP number 1 ranked players'
(would be 'Kirk Kerkorian', his middle name, if we include synthetic links)

 remove all templates 
  <template.*?</template>
 remove all sythentic links 
  <link synthetic="true">.*?</link> 
 first target
  andre agassi link removed since synthetic

structure was...
articles
 article
  paragraph
   sentence
    first target  

though i think just targets after removing templates is enough...

comparing with ayn_rand
 it would be 'American values' if we include 'link synthetic="true"'
 but 'novelist' if we exclude 'link synthetic="true"'

sometimes there iis text before the 1st paragraph, eg disambiguation info...
so need to trim to first paragraph too!!

final is then..

 ignore "articles" that have a name starting with 'File:', 'Template:',etc
 ignore "articles" that have plain text less than 30 chars

 trim to first paragraph
 remove all templates 
  <template.*?</template>
 remove all sythentic links 
  <link synthetic="true">.*?</link> 
 first target
  andre agassi link removed since synthetic

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /sample/articles/ -output /sample/edges \
  -mapper '/usr/bin/python articleParser.py' -file articleParser.py
  -numReduceTasks 0


no_outbound_link_found 15,491
plain_text_too_short   61,863	
metafile 	       1,480,805	
Map input records      4,183,153 	
Map output records     2,624,994 	  

looking through the diff of the from_nodes to the to_nodes in the edges
looks like we need to convert the to_nodes to their redirects

 cut -f1 all.edges | sort > all.edges.from             # 2624994 lines
 cut -f2 all.edges | uniq | sort | uniq > all.edges.to # 497461 lines

the redirects file isn't huge (3.3e6 rows, 154mb) so i thought it'd feasbile to do this in memory.
eg see dereferenceToLinks.py
but it's a complete fail. splitting into 16 chunks & running in parallel on cc1.4xlarge and it's a fail. 
(running 10+hrs and each derefernce dict lookup taking roughly 2s for every 3 records (?! that's a 45hr runtime)
i obviously need to learn me some more python...

the main reason i did this since i thought they'd be multiple redirects but looking at ~200e3 samples it's not the
case, if there is a redirect it's directly done

which means this is just a join; do it in pig

-- pig -p SET=sample|full -f edges_dereferenced.pig
edges = load '$SET/edges' as (from_node: chararray, to_node: chararray);
redirects_with_id = load '$SET/redirects' as (id:long, from_node: chararray, to_node: chararray);
redirects = foreach redirects_with_id generate from_node, to_node;
joined = join edges by to_node left outer, redirects by from_node;
edges_dereferenced = foreach joined generate
	edges::from_node as from_node, 
	(redirects::to_node is null ? edges::to_node : redirects::to_node) as to_node;
store edges_dereferenced into '$SET/edges_dereferenced';

takes a minute. though of course this is not a fundamental pig vs python thing, it's an algorithm difference.
a simpler merge approach could have been done in python must faster too i'm sure

 cut -f1 all.edges_dereferenced | sort > all.edges_dereferenced.from             # 2624994 lines (sanity)
 cut -f2 all.edges_dereferenced | uniq | sort | uniq > all.edges_dereferenced.to # 455224 lines



now we can work on calculating the distance for each page from 'Philosophy', and this is simply a breadth first search

again, trying to be pragmatic, i wrote a version in python (distanceFromPhilosophy.py) but my god it's slow...
?seconds for ? lookups in a dict? what am i missing?

Tue Aug  2 05:39:38 UTC 2011

=== move to newer version of the dump

mkfifo articles
hadoop fs -copyFromLocal articles /full/articles-2011-07-08/freebase-wex-2011-07-08-articles.tsv &
curl http://download.freebase.com/wex/latest/freebase-wex-2011-07-08-articles.tsv.bz2 | bunzip2 > articles &

hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
 -D mapred.min.split.size=300000000 \
 -input /full/2011-07-08/articles \
 -output /full/2011-07-08/edges \
 -mapper articleParser.py \
 -file articleParser.py \
 -numReduceTasks 0

Job Counters
Rack-local map tasks       0    0       189
Launched map tasks         0   0        190
Data-local map tasks       0   0        1
FileSystemCounters
HDFS_BYTES_READ            55,499,655,843       0       55,499,655,843
HDFS_BYTES_WRITTEN         129,764,139          0               129,764,139
parse
no_outbound_link_found     9,647        0       9,647
plain_text_too_short       87,599                               0       87,599
metafile                   1,896,275                            0       1,896,275
Map-Reduce Framework
Map input records          5,596,834    0       5,596,834
Spilled Records            0   0     0
Map input bytes            55,487,987,179       0       55,487,987,179
Map output records         3,603,313            0       3,603,313

#articles => 5,596,834
#edges extracted from articles 3,603,313

# edges.from              3603313
# edges.from (uniq)       3603249
# edges.to   (uniq)        684632

pig -p SET=/full/2011-07-08/ -f edges_dereferenced.pig

# edges_dereferenced.from        3603313 (sanity)
# edges_dereferenced.from (uniq) 3603249 (sanity)
# edges_dereferenced.from         621604 (not as many as the 2009-01-12 dataset...)

would downcasing help? maybe, but it's drifting further away from the true data

-- run the fscker!
 hfs -cat /full/2011-07-08/edges_dereferenced/* | java -classpath . com.Test "Philosophy" >edges 2>progress &

lots of examples that didn't work

eg truth, which is returning 'Fran√ßois Lemoyne' instead of 'Reality'

need another gold set to work with

under articles.eg
17th_Delaware_General_Assembly.eg
1949_Coupe_de_France_Final.eg
1999_Japan_Open_Tennis_Championships_Womens_Singles.eg
BAFC.eg
Bird_Gets_the_Worm.eg
Category_1022_books.eg
File_Pasquale_Caggiano_png.eg
Fort_Baxter.eg
Jinxiang_dialect.eg
Truth.eg

$ cat article.egs/* | ./articleParser.py  2>/dev/null
17th Delaware General Assembly		  Delaware Senate
1949 Coupe de France Final		  soccer
1999 Japan Open Tennis Championships ‚Äì Women's Singles	Ai Sugiyama
Bird Gets the Worm     Charlie Parker
Jinxiang dialect       People's Republic of China
Truth	 Fran√ßois Lemoyne

should be....
17th Delaware General Assembly		  Delaware Senate
1949 Coupe de France Final		  soccer
1999 Japan Open Tennis Championships ‚Äì Women's Singles	Ai Sugiyama
Bird Gets the Worm     Charlie Parker
Jinxiang dialect       Taihu Wu dialects ** different
Truth	 Fact ** different

(note: 
consider 'Jinxiang dialect'

$ cut -f4 article.egs/Jinxiang_dialect.eg | sed -es/\\\\n/\ /g | xmllint --format -

picked up target is from side bar...
.. <param name="states"><link><target>People's Republic of China</target></link> ..

correct target is later...
.. or a Northern <link synthetic="true"><target>Taihu Wu dialects</target><part>Wu dialect</part></link>, spoken in ..

$ cut -f5 article.egs/Jinxiang_dialect.eg | less
The<space/><bold><link synthetic="true"><target>1949 Coupe de France Final</target><part>Coupe de France Final</part></link> 1949</bold><space/>was a<space/><link><target>soccer</target><part>football</part></link>

plain text is 
Jinxiang dialect (ÈáëÈÑâË©±), is a Taihu Wu dialect, or a Northern Wu dialect, spoken in ...

so perhaps a better parsing strategy is
 extract all target linkes, href and link text
 choose the link target whose plain text appears first in the plain text

as a sanity check consider '1949 Coupe de France Final'

plain text is 
The Coupe de France Final 1949 was a football match held at Stade ...

going to need beautiful soup

 wget http://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.2.0.tar.gz
 tar zxf BeautifulSoup-3.2.0.tar.gz
 cd BeautifulSoup-3.2.0
 sudo python ./setup.py install

python
 from BeautifulSoup import BeautifulStoneSoup 
 f = open('article.egs/1949_Coupe_de_France_Final.xml','r')
 soup = BeautifulStoneSoup(f.read())
 links = soup.findAll('link')

then for
 <link synthetic="true"><target>1949 Coupe de France Final</target><part>Coupe de France Final</part></link>

>>> links[0].target
<target>1949 Coupe de France Final</target>
>>> links[0].target.string
u'1949 Coupe de France Final'
>>> links[0].part
<part>Coupe de France Final</part>
>>> links[0].part.string
u'Coupe de France Final'

and for
 <link><target>Stade Olympique Yves-du-Manoir</target></link>

>>> links[2].target
<target>Stade Olympique Yves-du-Manoir</target>
>>> links[2].target.string
u'Stade Olympique Yves-du-Manoir'
>>> links[2].part == None
True

for template in soup.findAll('template'):
  template.extract()

also needed to another heuristic which was to only examine the first 10 links 
(otherwise the link 'fact' deep in the truth article matched a 'fact' plain text at the start of the article)
feels dangerous...

also noticed that 1949_Coupe_de_France_Final.eg -> soccer 
and not Soccer as it should, and there is no redirect, and the current life page is correctly Soccer
might need to handle this in graph redirecting, if not present, and lower case, try upper case



17th Delaware General Assembly 	   Delaware Senate
1949 Coupe de France Final 	   soccer
1999 Japan Open Tennis Championships ‚Äì Women's Singles	Ai Sugiyama
Bird Gets the Worm     Charlie Parker
Brendan Foster 	       Order of the British Empire
Garh More 	       Jhang District
Harbour View, New Zealand    Lower Hutt
Jinxiang dialect  Taihu Wu dialects
Truth 	 Reality
  
 
restart another cluster from scratch

elastic-mapreduce --create --alive \
 --num-instances 5 --master-instance-type m1.large --slave-instance-type m1.large \
 --bootstrap-action s3://mkelcey/wikipediaPhilosophy/install_beautiful_soup.sh

then on master
 mkfifo articles
 hadoop fs -copyFromLocal articles /full/2011-07-23/articles/freebase-wex-2011-07-23-articles.tsv &
 curl -s http://download.freebase.com/wex/2011-07-23/freebase-wex-2011-07-23-articles.tsv.bz2 | bunzip2 > articles &
 mkfifo redirects
 hadoop fs -copyFromLocal redirects /full/2011-07-23/redirects/freebase-wex-2011-07-23-redirects.tsv &
 curl -s http://download.freebase.com/wex/2011-07-23/freebase-wex-2011-07-23-redirects.tsv.bz2 | bunzip2 > redirects &

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /full/2011-07-08/articles/ -output /full/2011-07-08/edges \
  -mapper '/usr/bin/python articleParser.py' -file articleParser.py

Job Counters 		   
Launched reduce tasks   0	0	10
Rack-local map tasks 	   0 	    0 	   94
Launched map tasks 	   0 	    0 	   423
Data-local map tasks 	   0 	    0 	   329
FileSystemCounters 	   
FILE_BYTES_READ	0	84,818,230	84,818,230
HDFS_BYTES_READ 	   55,520,558,984 	0 	55,520,558,984
FILE_BYTES_WRITTEN 	   104,093,878 		84,818,230	188,912,108
HDFS_BYTES_WRITTEN 	   0 			130,919,661 	130,919,661
parse 			   
10_links_examin		   1,918,250	0	1,918,250
no_match 		   189,136 	0 		189,136
exception 		   37,768 	0 		37,768
metafile 		   1,896,275 	0 		1,896,275
Map-Reduce Framework 	   
Reduce input groups 			0 		3,473,630	3,473,630
Combine output records 	   0 	  0 	0
Map input records 	   5,596,834 	0	5,596,834
Reduce shuffle bytes 	   0 		103,747,842	103,747,842
Reduce output records 	   0 		3,473,655 	3,473,655
Spilled Records 	   3,473,655 	3,473,655 	6,947,310
Map output bytes 	   130,919,781 	0 		130,919,781
Map input bytes 	   55,487,987,179 		0	55,487,987,179
Map output records 	   3,473,655 			0 	3,473,655
Combine input records 	   0 				0 	0
Reduce input records 	   0 				3,473,655	3,473,655

examining a random attempt
( /mnt/var/log/hadoop/userlogs/attempt_201107310225_0047_m_000017_0 )
on one of the slaves we see this breakdown
 cat stderr |sort|uniq -c
    849 parse exception <class 'sre_constants.error'>
    510 parse exception <type 'exceptions.TypeError'>

adjusted err output to include articles name and reran to grab some samples
 hfs -cat /full/2011-07-08/articles/freebase-wex-2011-07-08-articles.tsv | ./articleParser.py 2>&1 >/dev/null | grep -v ^reporter

made some fixes (robustness for character) and kicked off again...

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /full/2011-07-08/articles/ -output /full/2011-07-08/edges \
  -mapper '/usr/bin/python articleParser.py' -file articleParser.py

Job Counters 		  
Launched reduce tasks   0	0	10
Rack-local map tasks 	   0 	    0 	   200
Launched map tasks 	   0 	    0 	   448
Data-local map tasks 	   0 	    0 	   248
FileSystemCounters 	   
FILE_BYTES_READ	           0	86,274,065	86,274,065
HDFS_BYTES_READ 	   55,520,558,984 	0 	55,520,558,984
FILE_BYTES_WRITTEN 	   105,741,514 		86,274,065	192,015,579
HDFS_BYTES_WRITTEN 	   0 			132,365,083 	132,365,083
parse 			   
10_links_examined_limit    1,971,706	0	1,971,706
no_match 		   188,725 	0 		188,725
metafile 		   1,896,275 	0 		1,896,275
Map-Reduce Framework 	   
Reduce input groups 			0 		3,511,809	3,511,809
Combine output records 	   0 	  0 	0
Map input records 	   5,596,834 	0	5,596,834
Reduce shuffle bytes 	   0 		105,382,939	105,382,939
Reduce output records 	   0 		3,511,834 	3,511,834
Spilled Records 	   3,511,834 	3,511,834 	7,023,668
Map output bytes 	   132,365,209 	0 		132,365,209
Map input bytes 	   55,487,987,179 		0	55,487,987,179
Map output records 	   3,511,834 			0 	3,511,834
Combine input records 	   0 				0 	0
Reduce input records 	   0 				3,511,834	3,511,834

and then again on the newer dataset

 hadoop jar ~/contrib/streaming/hadoop-streaming.jar \
  -input /full/2011-07-23/articles/ -output /full/2011-07-23/edges \
  -mapper '/usr/bin/python articleParser.py' -file articleParser.py

